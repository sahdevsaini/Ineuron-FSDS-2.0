{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# ------------------------------- Assignment 19 ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "### Q1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ?\n",
    "1. Using the k-means method, create two clusters for each set of centroid described above.\n",
    "2. For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783f24c-12a7-44d5-a6c8-3ea432dac069",
   "metadata": {},
   "source": [
    "**Ans:** a) Cluster 1 (using 15, 32 as centroids): 5, 10, 15, 20, 25 // Cluster 2 (using 15, 32 as centroids): 30, 35\n",
    "\n",
    "b) For the set of centroid values 15, 32, the SSE = 15 + 25 = 40.\n",
    "For the set of centroid values 12, 30, the SSE = 10 + 25 = 35."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "### Q2. Describe how the Market Basket Research makes use of association analysis concepts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86218cb1-b307-4980-91ec-0b3051bb0888",
   "metadata": {},
   "source": [
    "**Ans: Market Basket Research (MBR)** is a data mining technique that uses association analysis concepts to identify patterns in customer behavior. `Association analysis` is used to `identify relationships` between **items that people purchase** together. `MBR` uses this information to identify frequent co-occurrences of items and to create “rules” that can be used to predict future customer behavior. MBR can be used to indicate which items should be **recommended to a customer based on their past purchases**, or to identify items that should be placed together in the same section of a store. In addition, it can provide insights into customer preferences, enabling retailers to tailor their marketing efforts and product offerings accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "### Q3. Give an example of the Apriori algorithm for learning association rules ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be201120-6d28-4f91-97a1-8e356f04e42c",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "Example:\n",
    "Let's say we have a dataset of items purchased in a grocery store. We can use the Apriori algorithm to find association rules that describe relationships between items.\n",
    "\n",
    "For example, we might find an association rule like:\n",
    "\n",
    "If a customer buys milk, they are likely to also buy eggs (support = 0.7, confidence = 0.8).\n",
    "\n",
    "This rule tells us that 70% of customers who buy milk also buy eggs and that 80% of customers who buy eggs also buy milk.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "### Q4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4145651-e2b1-4a27-b569-a47e05a79f80",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "In hierarchical clustering, the distance between clusters is measured using various metrics, with the choice depending on the specific application and the nature of the data. The most common distance metrics used in hierarchical clustering are:\n",
    "```\n",
    "**Euclidean Distance:** This is the most widely used distance metric, calculated as the straight-line distance between two points in Euclidean space. It is suitable for data with continuous features.<br>\n",
    "**Manhattan Distance:** Also known as the L1 norm or city block distance, it measures the sum of the absolute differences between the coordinates of two points. It is suitable for data with discrete or categorical features.<br>\n",
    "**Cosine Similarity:** This metric measures the cosine of the angle between two vectors, representing the similarity of their directions regardless of their magnitude. It is commonly used for text data or high-dimensional data.<br>\n",
    "**Correlation Distance:** This measures the correlation between two vectors, representing how much they vary together. It is commonly used for data with highly correlated features.<br>\n",
    "**Hamming Distance:** This is used for binary data and calculates the number of positions at which the corresponding bits are different between two binary vectors.<br>\n",
    "\n",
    "Once the distance metric is chosen, hierarchical clustering proceeds iteratively by merging clusters based on their proximity. At each iteration, the algorithm calculates the distance between all pairs of clusters and merges the closest pair into a single cluster. This process continues until all data points are in a single cluster (agglomerative clustering) or each data point is in its own cluster (divisive clustering).\n",
    "\n",
    "To decide when to end the iteration and stop merging clusters, hierarchical clustering algorithms often use linkage criteria, which define the distance between clusters. Common linkage criteria include:<br>\n",
    "\n",
    "**Single Linkage (Minimum Linkage):** Measures the distance between the closest points of two clusters. It tends to produce long, elongated clusters.<br>\n",
    "**Complete Linkage (Maximum Linkage):** Measures the distance between the farthest points of two clusters. It tends to produce compact, spherical clusters.<br>\n",
    "**Average Linkage:** Measures the average distance between all pairs of points in two clusters. It balances the effects of single and complete linkage.<br>\n",
    "**Centroid Linkage:** Measures the distance between the centroids of two clusters.<br>\n",
    "**Ward's Linkage:** Minimizes the increase in variance when merging two clusters.<br>\n",
    "\n",
    "The choice of linkage criterion affects the shape and structure of the resulting dendrogram, which visualizes the hierarchy of clusters. Hierarchical clustering algorithms typically stop merging clusters when a predefined number of clusters is reached (cutting the dendrogram at a certain height) or when the distance between clusters exceeds a specified threshold.\n",
    "\n",
    "In summary, the distance between clusters in hierarchical clustering is measured using various distance metrics, and the iteration is ended based on predefined stopping criteria such as the number of clusters, distance threshold, or linkage criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "### Q5. In the k-means algorithm, how do you recompute the cluster centroids ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d61ba-f552-4cc2-84fc-6617c39ca4b1",
   "metadata": {},
   "source": [
    "**Ans:** To recompute the cluster centroids, the mean of all data points in each cluster needs to be calculated. This mean is then used to update the centroid position. To do this, the coordinates of each data point in the cluster are added together and then divided by the number of data points in the cluster. This new mean value is then used as the new centroid position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "### Q6. At the start of the clustering exercise, discuss one method for determining the required number of clusters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555ce12-39e3-4c48-8008-d5162ba54cb0",
   "metadata": {},
   "source": [
    "**Ans:** One method for determining the required number of clusters is the **elbow method**. This method involves plotting the within-cluster sum of squared errors (WCSS) against the number of clusters and identifying the number of clusters where the WCSS begins to plateau. This is the point at which adding more clusters would not significantly improve the WCSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "### Q7. Discuss the k-means algorithm's advantages and disadvantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc5789-4107-475d-a2c7-dd3f3563eeac",
   "metadata": {},
   "source": [
    "**Ans:** Advantages of K-Means Algorithm: \n",
    "1. K-Means is an efficient and simple algorithm for clustering data. \n",
    "2. K-Means is robust to outliers and noise in the data.\n",
    "3. K-Means is relatively fast and computationally inexpensive.\n",
    "4. K-Means produces tighter clusters than hierarchical clustering.\n",
    "\n",
    "Disadvantages of K-Means Algorithm: \n",
    "1. K-Means requires prior knowledge of the number of clusters (K) to be generated.\n",
    "2. K-Means is sensitive to the initial seed points and may produce different results each time it is run.\n",
    "3. K-Means is not suitable for data with non-linear patterns or non-spherical distributions.\n",
    "4. K-Means is not suitable for data with outliers or noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "### Q8. Draw a diagram to demonstrate the principle of clustering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b9f6a-53bc-4f09-a450-6ea2eb89ff83",
   "metadata": {},
   "source": [
    "    ^      Cluster 1\n",
    "    | ●\n",
    "    |    ●\n",
    "    |       ●\n",
    "    |           ●\n",
    "    |                 ●\n",
    "    | ●                     ●\n",
    "    |          Cluster 2\n",
    "    |                       ●\n",
    "    |                                 ●\n",
    "    |                                            ●\n",
    "    |                                                          ●\n",
    "    |             Cluster 3       ●\n",
    "    +----------------------------------------------------------->\n",
    "                  Feature 1\n",
    " \n",
    " ```python\n",
    "  In this diagram:\n",
    "\n",
    "Each ● represents a data point in a two-dimensional feature space.\n",
    "The clusters are represented by groups of closely located data points.\n",
    "Data points within the same cluster are more similar to each other compared to data points in different clusters.\n",
    "The goal of clustering is to partition the data into clusters such that data points within each cluster are more similar to each other than to data points in other clusters.                \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "### Q9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration ?\n",
    "- `C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),  `\n",
    "- `C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,  `\n",
    "- `C3: (5,5) and (9,9) ` \n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb3ef9-a06e-48cd-92fd-00f3d37ee785",
   "metadata": {},
   "source": [
    "**Ans:** The cluster centroids for the second iteration would be: \n",
    "\n",
    "C1: (4, 4);\n",
    "\n",
    "C2: (2, 2);\n",
    "\n",
    "C3: (7, 7).\n",
    "\n",
    "The SSE for this clustering would be:\n",
    "\n",
    "SSE = (2 − 4)2 + (2 − 4)2 + (6 − 4)2 + (6 − 4)2 + (0 − 2)2 + (4 − 2)2 + (0 − 2)2 + (4 − 2)2 + (0 − 2)2 + (4 − 2)2 + (0 − 2)2 + (4 − 2)2 + (5 − 7)2 + (9 − 7)2 = 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "### Q10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae922e5-d0ab-4880-bf16-c10801ebc159",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Ans:** Step 1: Understanding the problem\n",
    "\n",
    "The problem is asking us to determine if software flaws discovered during testing are identical. We have 20 defect data points that are clustered into 5 clusters using the k-means algorithm. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. We need to explain this process using a simple diagram.\n",
    "\n",
    "Step 2: Understanding the k-means algorithm\n",
    "\n",
    "Before we proceed with the problem, let's understand the k-means algorithm. The k-means algorithm is an unsupervised machine learning algorithm used for clustering. It groups data points into k clusters based on their similarity. The algorithm starts by selecting k random centroids and assigns each data point to its nearest centroid. It then calculates the new centroids based on the mean of the data points in each cluster and repeats the process until the centroids no longer change.\n",
    "\n",
    "Step 3: Applying the k-means algorithm\n",
    "\n",
    "In our problem, we have 20 defect data points that are clustered into 5 clusters using the k-means algorithm. We can represent this as follows:\n",
    "    \n",
    "`Cluster 1: Defect data points 1, 4, 6, 9, 11`\n",
    "\n",
    "`Cluster 2: Defect data points 2, 7, 8, 14`\n",
    "\n",
    "`Cluster 3: Defect data points 3, 10, 12, 15, 18`\n",
    "\n",
    "`Cluster 4: Defect data points 5, 13, 16, 17, 19`\n",
    "\n",
    "`Cluster 5: Defect data point 20`\n",
    "\n",
    "we can see that the 20 defect data points have been grouped into 5 clusters based on their similarity using the k-means algorithm. Cluster 1 contains defect data points 1, 4, 6, 9, and 11, which are similar to each other. Cluster 2 contains defect data points 2, 7, 8, and 14, which are similar to each other. Cluster 3 contains defect data points 3, 10, 12, 15, and 18, which are similar to each other. Cluster 4 contains defect data points 5, 13, 16, 17, and 19, which are similar to each other. Finally, Cluster 5 contains only one defect data point, which means it is unique and not similar to any other defect.\n",
    "\n",
    "Step 4: Listing new defects\n",
    "\n",
    "As per the problem statement, any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. This means that if a new defect is discovered during testing, it should be assigned to one of the existing clusters based on its similarity to the defect data points in that cluster.\n",
    "\n",
    "For example, if a new defect is discovered during testing and it is similar to defect data points 1, 4, 6, 9, and 11, it should be assigned to Cluster 1. If it is similar to defect data points 2, 7, 8, and 14, it should be assigned to Cluster 2, and so on.\n",
    "\n",
    "Step 5: Conclusion\n",
    "\n",
    "In conclusion, we have solved the problem of determining if software flaws discovered during testing are identical. We used the k-means algorithm to cluster the defect data points into 5 clusters based on their similarity. We represented the clusters in a diagram and explained how to list any new defects that are discovered during testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efd989-4367-43b2-a945-ef30617b82da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
