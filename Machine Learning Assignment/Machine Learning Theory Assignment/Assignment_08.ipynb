{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# ----------------- Assignment 08 Solutions -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4f3bd",
   "metadata": {},
   "source": [
    "#### Q1. What exactly is a feature? Give an example to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1cdb",
   "metadata": {},
   "source": [
    "**Ans:**  feature refers to an individual measurable property or characteristic of the data that is used as input for a machine learning model. Features represent the variables or attributes that help the model understand and make predictions or classifications based on patterns in the data. \n",
    "```python\n",
    "Here's an example to illustrate the concept of features in machine learning:\n",
    "\n",
    "Let's say we have a dataset of houses with the following features:\n",
    "\n",
    "1. Number of bedrooms: The number of bedrooms in the house.\n",
    "2. Number of bathrooms: The number of bathrooms in the house.\n",
    "3. Location: The geographical location of the house.\n",
    "4. Age: The age of the house in years.\n",
    "5. Neighborhood crime rate: The crime rate in the neighborhood where the house is located.\n",
    "6. Square footage: The size of the house in square feet.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bc9d9",
   "metadata": {},
   "source": [
    "#### Q2. What are the various circumstances in which feature construction is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0d0e7",
   "metadata": {},
   "source": [
    "**Ans:-**\n",
    "```python\n",
    "Feature construction, also known as feature engineering, is the process of creating new features or transforming existing features in a dataset to improve the performance of machine learning models. Here are some circumstances in which feature construction is required:\n",
    "\n",
    "Insufficient or irrelevant features: Sometimes the dataset may lack certain features that are important for making accurate predictions. In such cases, new features need to be constructed from existing ones or external sources to capture the relevant information.\n",
    "\n",
    "Non-linearity in data: When the relationship between features and the target variable is non-linear, feature construction techniques such as polynomial features or interaction terms can be used to capture these non-linearities and improve model performance.\n",
    "\n",
    "Dimensionality reduction: High-dimensional datasets with a large number of features can suffer from the curse of dimensionality, leading to increased computational complexity and overfitting. Feature construction techniques such as principal component analysis (PCA) or manifold learning methods can be used to reduce the dimensionality of the dataset while preserving important information.\n",
    "\n",
    "Handling categorical variables: Categorical variables with multiple categories need to be encoded into numerical format for machine learning models to process them. Feature construction techniques such as one-hot encoding or ordinal encoding can be used to convert categorical variables into numerical features that can be used in models.\n",
    "\n",
    "Capturing temporal patterns: In time-series data or sequential data, feature construction may involve creating lag features, rolling statistics, or time-based aggregations to capture temporal patterns and trends in the data.\n",
    "\n",
    "Dealing with missing data: Missing values in the dataset can be problematic for machine learning models. Feature construction techniques such as imputation can be used to fill in missing values or create new features that indicate the presence of missing data.\n",
    "\n",
    "Improving model interpretability: Feature construction can also be used to create interpretable features that are easier to understand and explain. For example, creating binary flags or derived features based on domain knowledge can help make the model more interpretable.\n",
    "\n",
    "Overall, feature construction is a crucial step in the machine learning pipeline that aims to extract relevant information from the data and create informative features that enhance the performance and interpretability of machine learning models.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a7ce",
   "metadata": {},
   "source": [
    "#### Q3. Describe how nominal variables are encoded ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1d24",
   "metadata": {},
   "source": [
    "**Ans:** Nominal variables are categorical variables with no inherent order or ranking among their categories. Encoding nominal variables is the process of representing these categories as numerical values that machine learning algorithms can understand. Here are some common methods for encoding nominal variables\n",
    "```python\n",
    "1. One-Hot Encoding:\n",
    "     For example, if a nominal variable \"Color\" has categories {Red, Green, Blue}, it would be encoded as follows:\n",
    "\n",
    "Red: [1, 0, 0]\n",
    "Green: [0, 1, 0]\n",
    "Blue: [0, 0, 1]\n",
    "\n",
    "2. Label Encoding:\n",
    "    For example, if a nominal variable \"Gender\" has categories {Male, Female}, it would be encoded as:\n",
    "\n",
    "Male: 0\n",
    "Female: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4555",
   "metadata": {},
   "source": [
    "#### Q4. Describe how numeric features are converted to categorical features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ce760",
   "metadata": {},
   "source": [
    "**Ans:** Converting categorical features into numeric features using domain knowledge. For example, we are given a list of countries and say we know the distance to these countries from India then we can replace it with distance from India. So, every country can be represented as its distance from India."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109820",
   "metadata": {},
   "source": [
    "#### Q5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de65b0",
   "metadata": {},
   "source": [
    "**Ans:** Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51584401",
   "metadata": {},
   "source": [
    "#### Q6. When is a feature considered irrelevant? What can be said to quantify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a92af6",
   "metadata": {},
   "source": [
    "**Ans:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise. \n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21fc3f",
   "metadata": {},
   "source": [
    "#### Q7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce42e",
   "metadata": {},
   "source": [
    "**Ans:** If two features `{X1, X2}` are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features. \n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a123",
   "metadata": {},
   "source": [
    "#### Q8. What are the various distance measurements used to determine feature similarity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d5c4",
   "metadata": {},
   "source": [
    "**Ans:** Four of the most commonly used distance measures in machine learning are as follows: \n",
    "- Hamming Distance. \n",
    "- Euclidean Distance\n",
    "- Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6004e",
   "metadata": {},
   "source": [
    "#### Q9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39944ba",
   "metadata": {},
   "source": [
    "**Ans:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes.\n",
    "\n",
    "\n",
    "| Feature            | Euclidean Distance                                                   | Manhattan Distance                                                |\n",
    "|--------------------|----------------------------------------------------------------------|-------------------------------------------------------------------|\n",
    "| Formula            | \\( \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + \\ldots} \\)                  | \\( |x_2 - x_1| + |y_2 - y_1| + \\ldots \\)                          |\n",
    "| Calculation        | Uses squares and square roots                                        | Uses absolute differences                                        |\n",
    "| Representation     | Represents shortest path between two points                          | Represents distance along grid-like paths                       |\n",
    "| Geometry           | Represents straight-line distance                                    | Represents distance along grid-like paths                       |\n",
    "| Usage              | Used when actual distance matters                                    | Used in grid-based environments or when movement is restricted |\n",
    "| Application        | Geometric problems, physical distances                               | Route planning, K                                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5cfd2",
   "metadata": {},
   "source": [
    "#### Q10. Distinguish between feature transformation and feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfcff",
   "metadata": {},
   "source": [
    "**Ans:** Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
    "\n",
    "| Feature            | Feature Transformation                                        | Feature Selection                                      |\n",
    "|--------------------|---------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Definition         | Modifies or transforms the existing features                 | Selects a subset of the existing features              |\n",
    "| Purpose            | Enhances the existing features, making them more informative | Reduces the dimensionality of the feature space         |\n",
    "| Technique Examples | Scaling, normalization, polynomial features                   | Filter methods, wrapper methods, embedded methods      |\n",
    "| Impact             | Changes the representation of the data                        | Reduces the number of features used in modeling        |\n",
    "| Flexibility        | Provides flexibility in handling different types of features | Requires domain knowledge to choose relevant features  |\n",
    "| Data Utilization   | Uses all existing features                                   | Utilizes a subset of existing features for modeling   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea9fe9-d42f-4c41-b180-8387f9921a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
