{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# ----------------------- Assignment 22 ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "### Q1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2538de",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "Hybrid Model: A technique that combines two or more different machine learning models in some way. But we can't get  95 percent precision as all other models give different precision rate accuracy is differed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "### Q2. What's the difference between hard voting classifiers and soft voting classifiers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7a03e",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "Hard voting classifiers and soft voting classifiers are two ensemble learning techniques used in machine learning for combining predictions from multiple base models. The main difference between them lies in how they aggregate individual classifiers' outputs to make a final prediction.\n",
    "```\n",
    "**Hard Voting Classifiers:**<br>\n",
    "In hard voting, each individual classifier in the ensemble provides a single class prediction for a given input sample.\n",
    "The final prediction is determined by a simple majority vote among the individual classifier's predictions.\n",
    "The class with the most votes is selected as the final prediction for the ensemble.\n",
    "Hard voting is typically used when individual classifiers provide discrete class labels rather than probabilities.\n",
    "**Soft Voting Classifiers:**<br>\n",
    "In soft voting, each individual classifier estimates the probability of each class for a given input sample.\n",
    "These probability estimates are combined across all classifiers, often by taking a weighted average where more confident classifiers have higher weights.\n",
    "The final prediction is made based on the class with the highest sum of weighted probabilities across all classifiers.\n",
    "Soft voting is beneficial when individual classifiers can provide probability estimates rather than just discrete class labels.<Br>\n",
    "**In summary, the key distinction is that hard voting relies on discrete class predictions and a majority vote, while soft voting utilizes probability estimates from individual classifiers to make a more nuanced decision.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "### Q3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f89027",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "When sampling is performed without replacement, it is called pasting. In other words, both approaches are similar.In both cases you are sampling the training data to build multiple instances of a classifier.\n",
    "\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. It is the best starting point for understanding boosting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "### Q4. What is the advantage of evaluating out of the bag ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca29c4",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "The advantage of the OOB method is that it requires less computation and allows one to test the model as it is being trained.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "### Q5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810b087",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra\n",
    "Trees use the whole original sample. This may increase variance because bootstrapping makes it more diversified. \n",
    "\n",
    "Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. Extra Trees is much faster.\n",
    "\n",
    "This is because instead of looking for the optimal split at each node it does it randomly.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "### Q6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e6514",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "### Q7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebb680",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "```python\n",
    "If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could  also use early stopping to find the right number of predictors (you probably have too many)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab922b-4f62-49b8-8edc-916937b6e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
